import os
import sys
import json
import asyncio
import requests
from dataclasses import dataclass
from dotenv import load_dotenv
from supabase import create_client, Client
from sentence_transformers import SentenceTransformer

"""Code inspired from https://github.com/coleam00/ottomator-agents/blob/main/crawl4AI-agent/crawl_pydantic_ai_docs.py"""

load_dotenv()

supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Load Supabase
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# Load Embedding Model (local, multilingual)
embedding_model = SentenceTransformer("MULTILINGUAL_EMBEDDING")

# Ollama Model for title/summaries
OLLAMA_URL = os.getenv("OLLAMA_URL")
NEDER_MODEL = os.getenv("NEDER_MODEL")

@dataclass
class ProcessedChunk:
    chunk_nummer: int
    titel: str
    samenvatting: str
    inhoud: str
    metadata: Dict[str, Any]
    embedding: List[float]

def chunk_text(text: str, chunk_size: int = 5000) -> List[str]:
    """Split text into chunks, respecting code blocks and paragraphs."""
    chunks = []
    start = 0
    text_length = len(text)

    while start < text_length:
        # Calculate end position
        end = start + chunk_size

        # If we're at the end of the text, just take what's left
        if end >= text_length:
            chunks.append(text[start:].strip())
            break

        # Try to find a code block boundary first (```)
        chunk = text[start:end]
        code_block = chunk.rfind('```')
        if code_block != -1 and code_block > chunk_size * 0.3:
            end = start + code_block

        # If no code block, try to break at a paragraph
        elif '\n\n' in chunk:
            # Find the last paragraph break
            last_break = chunk.rfind('\n\n')
            if last_break > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size
                end = start + last_break

        # If no paragraph break, try to break at a sentence
        elif '. ' in chunk:
            # Find the last sentence break
            last_period = chunk.rfind('. ')
            if last_period > chunk_size * 0.3:  # Only break if we're past 30% of chunk_size
                end = start + last_period + 1

        # Extract chunk and clean it up
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)

        # Move start position for next chunk
        start = max(start + 1, end)

    return chunks

async def get_title_and_summary(chunk: str) -> Dict[str, str]:
    """Extract title and chunk_nummer using GPT-4."""
    
    system_prompt = """Je bent an AI die titels and samenvattingen ontrekt van delen van documenten.
    Return een JSON object met 'titel' en 'samenvatting' keys.
    Voor de titel: Als het lijkt op het begin van een document, ontrek de titel. Als het een middel deel is, leidt er dan een omschrijvende titel van af.
    Voor de samenvatting: Creeer een korte samenvatting van de kern van het deel.
    Hou de titel en samenvatting beide kort maar informatief."""
    
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{OLLAMA_URL}/v1/chat/completions",
                json={
                    "model": OLLAMA_MODEL,
                    "messages": [
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": 0.3,
                    "stream": False
                }
            ) as resp:
                data = await resp.json()
                content = data["choices"][0]["message"]["content"]
                return json.loads(content)
    except Exception as e:
        print(f"❌ Error generating title/summary with Ollama: {e}")
        return {"titel": "Fout", "samenvatting": "Fout"}


async def get_embedding(text: str) -> List[float]:
    try:
        return embedding_model.encode(text).tolist()
    except Exception as e:
        print(f"❌ Error generating embedding: {e}")
        return [0.0] * 384  # assuming MiniLM-L12-v2

async def process_chunk(chunk: str, chunk_nummer: int) -> ProcessedChunk:
    """Process a single chunk of text."""
    # Get title and summary
    extracted = await get_title_and_summary(chunk)
    
    # Get embedding
    embedding = await get_embedding(chunk)
    
    """Hier valt nog veel te halen. Denk aan metadata over het boek, de editie,
     vak (dingen voor later om rekening mee te houden)"""
    metadata = {
        "source": "leerboek_kgt_2_52-70",
        "chunk_size": len(chunk),
        "subject": "wiskunde",
        "niveau": "vmbo-tl",
        "taal": "nl",
        "boek_versie": "2024"
    }
    
    return ProcessedChunk(
        chunk_nummer=chunk_nummer,
        titel=extracted.get("titel", "Geen titel"),
        samenvatting=extracted.get("samenvatting", "Geen samenvatting"),
        inhoud=chunk,
        metadata=metadata,
        embedding=embedding
    )

async def insert_chunk(chunk: ProcessedChunk):
    """Insert a processed chunk into Supabase."""
    try:
        data = {
            "chunk_nummer": chunk.chunk_nummer,
            "titel": chunk.titel,
            "samenvatting": chunk.samenvatting,
            "inhoud": chunk.inhoud,
            "metadata": chunk.metadata,
            "embedding": chunk.embedding
        }
        
        result = supabase.table("site_pages").insert(data).execute()
        print(f"Inserted chunk {chunk.chunk_nummer}")
        return result
    except Exception as e:
        print(f"Error inserting chunk: {e}")
        return None

async def process_and_store_document(markdown: str):
    """Process a document and store its chunks in parallel."""
    # Split into chunks
    chunks = chunk_text(markdown)
    
    # Process chunks in parallel
    tasks = [
        process_chunk(chunk, i) 
        for i, chunk in enumerate(chunks)
    ]
    processed_chunks = await asyncio.gather(*tasks)
    
    # Store chunks in parallel
    insert_tasks = [
        insert_chunk(chunk) 
        for chunk in processed_chunks
    ]
    await asyncio.gather(*insert_tasks)